{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838bf48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go one level up in the directories hierarchy to access src directory and codes\n",
    "import sys\n",
    "import os\n",
    "# Add project root to Python path\n",
    "project_root = os.path.abspath(\"..\")  # go one level up from notebooks/\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "888547db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 10:21:14,418 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite \"HTTP/1.1 200 OK\"\n",
      "2025-12-11 10:21:14,421 - INFO - Load pretrained SentenceTransformer: intfloat/multilingual-e5-small\n"
     ]
    }
   ],
   "source": [
    "# Setup necessary models for chatting and embedding\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from src.config import Config\n",
    "from google.genai import types\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model = Config.CHAT_LLM,\n",
    "    api_key = Config.GOOGLE_API_KEY,\n",
    "    generation_config = types.GenerateContentConfig(\n",
    "        thinking_config = types.ThinkingConfig(thinking_budget = 0),\n",
    "        temperature = 0.2,\n",
    "    ),\n",
    "    max_tokens = 3000\n",
    ")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name = Config.EMBEDDING_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c50496c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 23/23 [00:00<00:00, 1279.43it/s]\n",
      "Generating embeddings: 100%|██████████| 30/30 [00:01<00:00, 17.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Setup simple RAG\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "docs_path = \"../documents\"\n",
    "\n",
    "# 1) Read documents and create list of 'Document' objects, that has id_, metadata, text attributes.\n",
    "#    Document class (generic container for any data source) is a subclass of the TextNode class \n",
    "documents = SimpleDirectoryReader(input_dir = docs_path).load_data()\n",
    "\n",
    "# 2) Read each of this document objects and create index from it\n",
    "#    Document objects are parsed into Node objects that have different attributes such as text, embeddings, metadata, relationships.\n",
    "#    Document objects are split into multiple nodes (relationships between these nodes are recorded in Node objects as attributes).\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents = documents,\n",
    "    embed_model = embed_model,\n",
    "    show_progress = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65b8bf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 10:21:23,926 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-12-11 10:21:26,089 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer model utilizes multi-head attention in three distinct ways. In \"encoder-decoder attention\" layers, queries originate from the preceding decoder layer, while keys and values are sourced from the encoder's output. This configuration enables every position in the decoder to examine all positions within the input sequence, mirroring conventional encoder-decoder attention mechanisms found in sequence-to-sequence models.\n",
      "\n",
      "The encoder incorporates self-attention layers where keys, values, and queries all stem from the output of the encoder's prior layer. This allows each position in the encoder to attend to all positions in the preceding encoder layer.\n",
      "\n",
      "Similarly, self-attention layers within the decoder permit each position to attend to all preceding positions in the decoder, including itself. To maintain the auto-regressive property, the model prevents information flow from right to left in the decoder by masking out (setting to negative infinity) values in the softmax input that correspond to invalid connections.\n",
      "\n",
      "The model employs eight parallel attention layers, or heads, with each head using a dimension of 64 for keys and values, and 64 for the model's dimension divided by the number of heads. This reduced dimensionality per head results in a computational cost comparable to single-head attention with full dimensionality.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 3) On top of that index build query engine for retrieving the context.\n",
    "query_engine = index.as_query_engine(llm = llm)\n",
    "\n",
    "# 4) Take user query and generate an answer\n",
    "user_query = \"Tell me about attention block in LLMs briefly\"\n",
    "response = query_engine.query(user_query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
